{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp_DNu6n7sKW"
      },
      "source": [
        "## Important Corrections\n",
        "1. The performance measure is **mean squared error**.\n",
        "2. The bias term was added to the bottom exercise.\n",
        "3. A completed version of the Linear Regression exercise was added.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPnwA9zSsXg6"
      },
      "source": [
        "# ACM Research Week 2: Linear Regression\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ACM-Research/image-classification-cnn/blob/master/week2/LinearRegression_Complete.ipynb)\n",
        "\n",
        "\n",
        "## Background\n",
        "### Dot Product\n",
        "Assume we have two vectors $\\vec{v} = (v_1, v_2, ..., v_n)$ and $\\vec{w} = (w_1, w_2, ..., w_n)$.\n",
        "\n",
        "The *dot product* is defined as the sum of the product of corresponding components:\n",
        "\n",
        "$$\\vec{v}\\cdot\\vec{w} = v_1 w_1 + v_2 w_2 + ... + v_n w_n$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2GjF330sT0P",
        "outputId": "3c00c2f2-47c5-4df4-f1e4-ca7cf4612f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "v = np.array([1, 2])\n",
        "w = np.array([3, 4])\n",
        "np.dot(v, w)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8tadAO2TDOt"
      },
      "source": [
        "### Matrix Multiplication\n",
        "Let's say have two matrices $A$ and $B$ as defined below:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix},\\:\\:\n",
        "B = \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The *product* $AB$ of two matrices is defined as the dot product between the rows of $A$ with the columns of $B$:\n",
        "\n",
        "$$AB = \\begin{bmatrix}\n",
        "1(7) + 2(9) + 3(11) & 1(8) + 2(10) + 3(12) \\\\\n",
        "4(7) + 5(9) + 6(11) & 4(8) + 5(10) + 6(12)\n",
        "\\end{bmatrix} = \\begin{bmatrix}58 & 64 \\\\ 139 & 154\\end{bmatrix}$$\n",
        "\n",
        "Note than if $A$ is of $a \\times b$ and $B$ is of $b \\times c$, then $AB$ is of size $a \\times c$. \n",
        "\n",
        "The product also doesn't exist if the number of rows in A doesn't match the number of columns in C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jWU5NgpTQ8Z",
        "outputId": "2fbe0284-a457-4712-84da-bbbdb41032f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "B = np.array([[7, 8], [9, 10], [11, 12]])\n",
        "np.dot(A, B)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 58,  64],\n",
              "       [139, 154]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB89COaDTRUx"
      },
      "source": [
        "### Transpose a Matrix\n",
        "Rows <-> columns. That's it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKWdoDQOTSRC",
        "outputId": "68a67a3d-aef6-4764-d061-0ac6c5dd9521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "A.T"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 4],\n",
              "       [2, 5],\n",
              "       [3, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bok2KFgKTTEY"
      },
      "source": [
        "## Main Content\n",
        "\n",
        "### Intro\n",
        "\n",
        "**Linear regression** is one of the most basic machine learning tasks: fit a linear model to some data.\n",
        "\n",
        "You might have seen a line in 2D as $y=mx+b$, where $m$ is the slope and $b$ is the y-intercept.\n",
        "\n",
        "Let's first *vectorize*, this, i.e. think of it within the realm of matrix multiplication. Instead of writing our line as $y=mx+b$, we can rewrite the right side of that equation as $y = w^{\\top}x$, where $x_0 = 1$ and $w$ is a $1\\times 2$ vector.\n",
        "\n",
        "As an example, how would we write the equation $y=3x+4$ in this form?\n",
        "Well, $w = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}$, and $x = \\begin{bmatrix} 1 \\\\ x_1 \\end{bmatrix}$, where $x_1$ takes the place of $x$ in the slope-intercept form equation. We can verify this is the same by multiplying it out:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "y &= w^{\\top} x \\\\\n",
        "y &= \\begin{bmatrix} 4 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ x_1 \\end{bmatrix} \\\\\n",
        "y &= 4 + 3x_1 \\;\\checkmark\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV9Q_YJGTXtp",
        "outputId": "42231c69-7d35-410f-edea-bfde5fdbe644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def equation(x1):\n",
        "  w = np.array([[4], [3]])\n",
        "  x = np.array([[1], [x1]])\n",
        "  return np.dot(w.T, x)\n",
        "print(equation(1).flatten())\n",
        "print(equation(4).flatten())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7]\n",
            "[16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGd2uovYTZBn"
      },
      "source": [
        "### Multivariate Linear Equations: Vectorized\n",
        "Now, let's extend this concept to multiple dimensions and multiple points.\n",
        "Let's say we had the equation $y = w_0x_0 +w_1 x_1 + w_2 x_2 + ... + w_n x_n$. This is a line in $n$ dimensions—for us, we'll say this model takes $n$ *features*. How would we vectorize this, and use it for multiple points?\n",
        "\n",
        "Let's say we have the **design matrix**:\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & 0.315 & 0.083 & 0.849 \\\\\n",
        "1 & 0.522 & 0.079 & 0.530 \\\\\n",
        "1 & 0.558 & 0.252 & 0.958 \\\\\n",
        "1 & 0.134 & 0.241 & 0.473\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Here, we have 3 *features* and 4 *data points*/*training examples*. We can also say $n=3$ and $m=4$.\n",
        "\n",
        "Let's say we also have a vector of *weights*:\n",
        "\n",
        "$$ w = \n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "w_2 \\\\\n",
        "w_3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.012 \\\\\n",
        "0.951 \\\\\n",
        "0.301 \\\\\n",
        "0.868\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Therefore, the vectorized version of the multivariate linear equation above would be:\n",
        "\n",
        "$$ y = Xw $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLwxCmg5TeeV"
      },
      "source": [
        "X = np.array([\n",
        "     [1, 0.315, 0.083, 0.849],\n",
        "     [1, 0.522, 0.079, 0.530],\n",
        "     [1, 0.558, 0.252, 0.958],\n",
        "     [1, 0.134, 0.241, 0.473]\n",
        "])\n",
        "w = np.array([[0.012], [0.951], [0.301], [0.868]])\n",
        "y = np.dot(X, w)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgmxNpWwTe97"
      },
      "source": [
        "### The Problem of Linear Regression\n",
        "With this knowledge, we can now talk about multivariate linear regression.\n",
        "\n",
        "> **The task $T$**: With equation $\\hat{y} = Xw$, have $\\hat{y} \\approx y$ for any $X$ by finding the optimal $w$.\n",
        "\n",
        "> **The experience $E$**: $X_{train}$, as provided\n",
        "\n",
        "> **Performance measure $P$**: <mark>Average (Euclidean) distance</mark> from $\\hat{y}$ to $y$\n",
        "\n",
        "Let's define all of these in terms of matrices and matrix equations:\n",
        "\n",
        "- The task $T$: $\\hat{y} = Xw$, $w \\in \\mathbb{R}^n$\n",
        "- The experience $E$: design matrix of data points\n",
        "- <mark>The performance measure $P$:</mark> $J(w) = \\frac{1}{m}||\\hat{y}- y||^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "375TBdpuUk74",
        "outputId": "17ad1d3c-ccba-463f-f291-dcd2217aefd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y_hat = np.array([[1.1283],\n",
        "       [1.0000],\n",
        "       [1.43],\n",
        "       [0.6234]])\n",
        "print((1/y.shape[0]) * np.linalg.norm(y_hat - y) ** 2.0)\n",
        "y_hat = np.array([[1.07348 ],\n",
        "       [0.992241],\n",
        "       [1.450054],\n",
        "       [0.622539]])\n",
        "print((1/y.shape[0]) * np.linalg.norm(y_hat - y) ** 2.0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0008670846795000091\n",
            "1.540743955509789e-32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP3p_V8rUla-"
      },
      "source": [
        "### Gradient Descent\n",
        "How do we use $P$ to get better at $T$?\n",
        "\n",
        "The answer is **gradient descent**. The basic idea behind gradient descent is to use the *partial derivative* of a function to iteratively converge to a (ideally global) minimum.\n",
        "\n",
        "The since this is vectorized, we need to find the partial derivative with respect to every parameter in $w$. In linear algebra, this is also called the gradient $\\nabla$. The gradient of the MSE function is $$\\nabla J(w) = \\frac{1}{m} X^{\\top}(Xw - y)$$\n",
        "\n",
        "So how do we update the weights $w$ given the gradient? Well, we multiply the gradient by a small decimal $\\alpha$ (called the *learning rate*), which is usually set to something small like `0.001` and subtract it from the existing weights.\n",
        "\n",
        "> **Note**: The learning rate will be different for every dataset and ML algorithm. Set it too small, and your model will take forever to *converge* (reach the minimum). Set it too high, and it may overshoot the minimum and fail to converge.\n",
        "\n",
        "As a matrix equation, it looks something like this:\n",
        "\n",
        "$$w = w - \\frac{\\alpha}{m}X^{\\top}(Xw - y)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scInKKN2U_cN"
      },
      "source": [
        "X = np.array([\n",
        "     [1, 0.315, 0.083, 0.849],\n",
        "     [1, 0.522, 0.079, 0.530],\n",
        "     [1, 0.558, 0.252, 0.958],\n",
        "     [1, 0.134, 0.241, 0.473]\n",
        "])\n",
        "w = np.array([[0.012], [0.951], [0.301], [0.868]])\n",
        "y_hat = np.dot(X, w)\n",
        "y = np.array([[1.5],\n",
        "       [0.23],\n",
        "       [1.0],\n",
        "       [0.6]])\n",
        "alpha = 0.001\n",
        "m = 4\n",
        "# iteratively do gradient descent!"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Og96C_VAMD"
      },
      "source": [
        "## Practice: Implementing a Linear Regression Model\n",
        "Steps to most ML projects:\n",
        "1. Pull in data\n",
        "2. Initialize weights\n",
        "3. Run gradient descent iteratively\n",
        "4. Test on new data\n",
        "5. Report results\n",
        "\n",
        "[Dataset!](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YURiRRAxVtAW"
      },
      "source": [
        "### Step 1: Pull in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaqKTuisVnK1",
        "outputId": "84cd3331-3d66-4c31-94ef-b7fcbd297ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "# Download data and import as pandas df\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "data = pd.read_csv(data_url, sep=\"\\s+\", header=None, na_values=['?'])\n",
        "# Remove column with car name\n",
        "del data[8]\n",
        "# Fill N/A values\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Helper function to do some work for us\n",
        "def split_train_test(df, train_split = 0.8, random_seed = 256):\n",
        "  # Find the index we need to go until to grab training rows\n",
        "  num_train_rows = math.floor(train_split * len(data))\n",
        "  # Shuffle data\n",
        "  shuffled_data = df.sample(frac=1, random_state=random_seed)\n",
        "  # Grab training/test df and ensure row-level indices stay the same\n",
        "  train_df = shuffled_data[:num_train_rows].reset_index(drop=True)\n",
        "  test_df = shuffled_data[num_train_rows + 1:].reset_index(drop=True)\n",
        "  # Split x and y values\n",
        "  train_x = train_df.loc[:, 1:] # get all columns except the one w/ index 0 (mpg)\n",
        "  train_y = train_df.loc[:, 0]  # get 0th column (mpg)\n",
        "  test_x = test_df.loc[:, 1:] # get all columns except the one w/ index 0 (mpg)\n",
        "  test_y = test_df.loc[:, 0]  # get 0th column (mpg)\n",
        "  # Insert column of 1s to x values to make this a design matrix\n",
        "  train_x.insert(0, 'bias', 1)\n",
        "  test_x.insert(0, 'bias', 1)\n",
        "  # Return tuple\n",
        "  return train_x.to_numpy(), train_y.to_numpy(), test_x.to_numpy(), test_y.to_numpy()\n",
        "\n",
        "train_x, train_y, test_x, test_y = split_train_test(data)\n",
        "# Note that train_x and test_x have that extra column which is used for the bias\n",
        "print(f'Training set input shape (rows x columns) is {train_x.shape}')\n",
        "print(f'Test set input shape (rows x columns) is {test_x.shape}')\n",
        "print(f'Training set output shape (rows x columns) is {train_y.shape}')\n",
        "print(f'Test set output shape (rows x columns) is {test_y.shape}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set input shape (rows x columns) is (318, 8)\n",
            "Test set input shape (rows x columns) is (79, 8)\n",
            "Training set output shape (rows x columns) is (318,)\n",
            "Test set output shape (rows x columns) is (79,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIMQFgI6bY5k"
      },
      "source": [
        "## Step 2: Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FshluaffbZP_"
      },
      "source": [
        "np.random.seed(256) # use a different seed to get diff results, or comment out to use random seed\n",
        "# (n+1) parameters needed, where n is the number of columns in original dataset\n",
        "w = np.random.random_sample((8,))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCtLWB12bc99"
      },
      "source": [
        "# Step 3: Run Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxAlYaPobrG7",
        "outputId": "f578afab-d802-480b-daba-d85237df5734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import math\n",
        "# Helper function\n",
        "def add_column_of_ones(arr):\n",
        "  m = arr.shape[0]\n",
        "  ones = np.zeros((m,1)) + 1\n",
        "  return np.c_[ones, arr]\n",
        "\n",
        "# Let's define a function predict which outputs matrix of predictions given X\n",
        "def predict(x_values, weights, is_design_matrix = True):\n",
        "  X = x_values\n",
        "  if not is_design_matrix:\n",
        "    # Add column of 1s to numpy array\n",
        "    X = add_column_of_ones(X)\n",
        "  return np.squeeze(np.dot(X, w))\n",
        "\n",
        "# Let's then define the cost function J(w)\n",
        "def cost(pred, actual):\n",
        "  m = pred.shape[0]\n",
        "  return (1/m) * (np.linalg.norm(pred - actual) ** 2.0)\n",
        "\n",
        "# Now, let's define the gradient of the cost function:\n",
        "def grad_cost(x_values, weights, actual, is_design_matrix = True):\n",
        "  X = x_values\n",
        "  m = X.shape[0]\n",
        "  if not is_design_matrix:\n",
        "    # Add column of 1s to numpy array\n",
        "    X = add_column_of_ones(X)\n",
        "  # Xw - y term\n",
        "  pred_diff = predict(X, weights) - actual\n",
        "  return (1/m) * np.dot(X.T, pred_diff)\n",
        "\n",
        "# Weight update rule\n",
        "def update_weights(alpha, weights, grad):\n",
        "  return weights - (alpha * grad)\n",
        "\n",
        "\n",
        "\n",
        "# This is an example of GD with stopping at a specific # of iterations\n",
        "# See https://stats.stackexchange.com/questions/33136/how-to-define-the-termination-condition-for-gradient-descent\n",
        "# for other options\n",
        "\n",
        "costs = []\n",
        "alpha = 2e-7 # This is a hyperparameter — you may need to play around with this\n",
        "iter = 0\n",
        "\n",
        "while iter < 1e6:\n",
        "  grad = grad_cost(train_x, w, train_y)\n",
        "  w = update_weights(alpha, w, grad)\n",
        "  cost_value = cost(predict(train_x, w), train_y)\n",
        "  costs.append(cost_value)\n",
        "  if iter % 100000 == 0:\n",
        "    print(f'Iteration {iter} - cost (mse): {cost_value}')\n",
        "  iter += 1\n",
        "print('Trained.')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 - cost (mse): 18484.792859233763\n",
            "Iteration 100000 - cost (mse): 13.273020197282872\n",
            "Iteration 200000 - cost (mse): 13.179284228984105\n",
            "Iteration 300000 - cost (mse): 13.097661623793005\n",
            "Iteration 400000 - cost (mse): 13.026320603537709\n",
            "Iteration 500000 - cost (mse): 12.963850857465355\n",
            "Iteration 600000 - cost (mse): 12.909037152243927\n",
            "Iteration 700000 - cost (mse): 12.860832172000029\n",
            "Iteration 800000 - cost (mse): 12.818333298137821\n",
            "Iteration 900000 - cost (mse): 12.780762600516944\n",
            "Trained.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1XhYfSibrXV"
      },
      "source": [
        "## Step 4: Test on New Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SpySiFhbuWo",
        "outputId": "4ca0b54f-3afc-4b77-d18a-a9a1cc72776c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mean squared error between predicted and actual\n",
        "cost(predict(test_x, w), test_y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.230436023386723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q2hVkOSbwYy"
      },
      "source": [
        "## Step 5: Report Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb4DikHIbyfP",
        "outputId": "a57e5574-be37-4565-d7a0-5038f8f2d6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Label axes and title\n",
        "plt.title('Cost over iterations (training)')\n",
        "plt.ylabel('J(w)')\n",
        "plt.xlabel('Iteration #')\n",
        "\n",
        "# Set log scale\n",
        "plt.xscale('log')\n",
        "\n",
        "# Plot J(w) over iterations\n",
        "plt.plot(np.arange(len(costs)), costs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd023ac23c8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkXXh1ka5Uxa",
        "outputId": "af61a531-46c7-4cfa-e2b8-edcdc00d0e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Let's pick a car not in the dataset: The 1999 Honda Accord.\n",
        "y_sample = np.array([[31]])\n",
        "x_sample = np.array([[4, 109.6, 158, 2885, 16.3, 99, 3]])\n",
        "y_pred = predict(x_sample, w, is_design_matrix=False).item()\n",
        "print(f'The 19{int(x_sample[0,5])} Honda Accord is a {y_sample.item()} MPG sedan with a {int(x_sample[0,0])}-cyl engine (among other properties).\\nThe model predicted this car to have {y_pred} MPG fuel economy.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 1999 Honda Accord is a 31 MPG sedan with a 4-cyl engine (among other properties).\n",
            "The model predicted this car to have 36.28047653346874 MPG fuel economy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UW4SovdArD7"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}