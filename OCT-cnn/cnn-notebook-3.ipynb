{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pathlib\nimport random\nimport glob\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras import layers\nimport PIL\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimages_path = os.path.join(os.path.realpath('..'), \"input\", \"kermany2018\", \"OCT2017 \")\nimages_path = pathlib.Path(images_path)\n\nimage_count = len(list(images_path.glob('*/*.jpeg')))\nprint(image_count)\n\nbatch_size = 128\nimg_height = 180\nimg_width = 180\n\n\ntest_ds = image_dataset_from_directory(\nos.path.join(images_path, \"test\"),\n  validation_split=0,\n  seed=1232376,\n  image_size=(img_height, img_width),\n  batch_size=128128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_ds = image_dataset_from_directory(\nos.path.join(images_path, \"val\"),\n  validation_split=0,\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = image_dataset_from_directory(\n  os.path.join(images_path, \"train\"),\n  validation_split=0,\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"num_classes = 4\n\nmodel = tf.keras.Sequential([\n  layers.experimental.preprocessing.Rescaling(1./255),\n  layers.Conv2D(32, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(256, activation='relu'),\n  layers.Dense(512, activation='relu'),\n  layers.Dense(256, activation='relu'),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])\n\nmodel.compile(\n  optimizer='adam',\n  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=3\n)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate F1 score\nfrom sklearn import metrics\n# metrics.f1_score(y_actual, y_pred, labels=[0, 1, 2], average='macro')\ny_pred =[]\ny_true = []\n\ni = 0\nfor batch, lbls in test_ds.as_numpy_iterator():\n    prediction = model.predict_on_batch(batch)\n    \n    for i in range(len(prediction)):\n        y_true.append(lbls[i])\n        prediction_single = np.argmax(prediction[i])\n        y_pred.append(prediction_single)\n#     print(batch.shape)\n#     print(lbls.shape)\nprint(y_true)\nprint(y_pred)\n    \nprint(metrics.f1_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}