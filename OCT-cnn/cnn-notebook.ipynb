{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pathlib\nimport random\nimport glob\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom keras.preprocessing import image_dataset_from_directory\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport PIL\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimages_path = os.path.join(os.path.realpath('..'), \"input\", \"kermany2018\", \"OCT2017 \")\nimages_path = pathlib.Path(images_path)\n\nimage_count = len(list(images_path.glob('*/*.jpeg')))\nprint(image_count)\n\n\ntest_ds = image_dataset_from_directory(\nos.path.join(images_path, \"test\"),\n  validation_split=0,\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\n\n","execution_count":7,"outputs":[{"output_type":"stream","text":"0\nFound 968 files belonging to 4 classes.\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"'\\nval_ds = image_dataset_from_directory(\\nos.path.join(images_path, \"val\"),\\n  validation_split=0,\\n  seed=123,\\n  image_size=(img_height, img_width),\\n  batch_size=batch_size)\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_ds = image_dataset_from_directory(\nos.path.join(images_path, \"val\"),\n  validation_split=0,\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","execution_count":10,"outputs":[{"output_type":"stream","text":"Found 32 files belonging to 4 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nimg_height = 180\nimg_width = 180\n\n\ntrain_ds = image_dataset_from_directory(\n  os.path.join(images_path, \"train\"),\n  validation_split=0,\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","execution_count":3,"outputs":[{"output_type":"stream","text":"Found 83484 files belonging to 4 classes.\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"num_classes = 4\n\nmodel = tf.keras.Sequential([\n  layers.experimental.preprocessing.Rescaling(1./255),\n  layers.Conv2D(32, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])\n\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=['accuracy'])","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=1\n)\n\nmodel.summary()","execution_count":8,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n2609/2609 [==============================] - 409s 157ms/step - loss: 0.5474 - accuracy: 0.7982 - val_loss: 0.2715 - val_accuracy: 0.9122\nEpoch 2/3\n2609/2609 [==============================] - 258s 99ms/step - loss: 0.2864 - accuracy: 0.9000 - val_loss: 0.1444 - val_accuracy: 0.9504\nEpoch 3/3\n2609/2609 [==============================] - 258s 99ms/step - loss: 0.1918 - accuracy: 0.9338 - val_loss: 0.0946 - val_accuracy: 0.9618\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrescaling (Rescaling)        (None, 180, 180, 3)       0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 178, 178, 32)      896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 87, 87, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 43, 43, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 41, 41, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 20, 20, 32)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 12800)             0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               1638528   \n_________________________________________________________________\ndense_1 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 1,658,436\nTrainable params: 1,658,436\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_ds)","execution_count":11,"outputs":[{"output_type":"stream","text":"1/1 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9375\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[0.09567084908485413, 0.9375]"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}